import logging
import gc
import torch
import torch.nn.functional as F
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from modelscope import AutoModel
from vllm import LLM
from vllm.distributed.parallel_state import destroy_model_parallel

# --------------------- æ—¥å¿—é…ç½® ---------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --------------------- FastAPI åˆå§‹åŒ– ---------------------
app = FastAPI(
    title="Hybrid Reranker API",
    description="åŸºäº KaLM-Embedding-2.5 ä¸ Jina-Reranker-v3 çš„ä¸¤é˜¶æ®µæ–‡æ¡£æ’åºæ¥å£",
)

# --------------------- è¯·æ±‚ä¸å“åº”æ¨¡å‹ ---------------------
class HybridRerankRequest(BaseModel):
    task: str
    query: str
    documents: List[str]
    topk: Optional[int] = 5
    threshold: Optional[float] = 0.6
    normalize: Optional[bool] = True

class HybridRerankResponse(BaseModel):
    ranked_documents: List[Dict]
    total_count: int

# ==========================================================
#                   æ¨¡å‹1ï¼šEmbedding ç²—æ’
# ==========================================================
class EmbeddingReranker:
    def __init__(self):
        self.model = None
        self.initialize()

    def initialize(self):
        try:
            logger.info("ğŸš€ åŠ è½½ KaLM-Embedding-2.5 æ¨¡å‹...")
            num_gpus = torch.cuda.device_count()
            self.model = LLM(
                model="/root/cyx/model_weights/KaLM-Embedding-2.5",  # KaLMæ¨¡å‹è·¯å¾„
                task="embed",
                tensor_parallel_size=num_gpus if num_gpus > 0 else 1,
                gpu_memory_utilization=0.15,
                trust_remote_code=True,
                dtype="float16",
            )
            logger.info("âœ… KaLM-Embedding æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ KaLM-Embedding åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def embed(self, task: str, texts: List[str], normalize: bool = True) -> torch.Tensor:
        """ç”Ÿæˆæ–‡æœ¬embedding"""
        try:
            inputs = [f"Instruct: {task}\nQuery: {t}" for t in texts]
            outputs = self.model.embed(inputs)
            embeddings = [o.outputs.embedding for o in outputs]
            emb_tensor = torch.tensor(embeddings)
            if normalize:
                emb_tensor = F.normalize(emb_tensor, p=2, dim=1)
            return emb_tensor
        except Exception as e:
            logger.error(f"ç”Ÿæˆembeddingå¤±è´¥: {e}")
            raise

    def rank_documents(
        self, task: str, query: str, documents: List[str], normalize: bool = True
    ) -> List[Dict]:
        """æ ¹æ®embeddingç›¸ä¼¼åº¦è¿›è¡Œç²—æ’"""
        if not documents:
            return []
        inputs = [f"Instruct: {task}\nQuery: {query}"] + documents
        embeddings = self.embed(task, inputs, normalize)
        query_emb = embeddings[0].unsqueeze(0)
        doc_embs = embeddings[1:]
        scores = (query_emb @ doc_embs.T).squeeze(0).tolist()

        ranked = sorted(
            [{"document": doc, "score": float(score)} for doc, score in zip(documents, scores)],
            key=lambda x: x["score"],
            reverse=True,
        )
        return ranked

# ==========================================================
#                   æ¨¡å‹2ï¼šReranker ç²¾æ’ (Jina-Reranker-v3)
# ==========================================================
class JinaReranker:
    def __init__(self):
        self.model = None
        self.initialize()

    def initialize(self):
        try:
            logger.info("ğŸš€ åŠ è½½ Jina-Reranker-v3 æ¨¡å‹...")
            # æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæ·»åŠ local://å‰ç¼€é¿å…æ ¼å¼æ ¡éªŒé”™è¯¯ï¼‰
            model_path = "/root/cyx/model_weights/jina-reranker-v3"
            
            self.model = AutoModel.from_pretrained(
                model_path,
                dtype="auto",
                trust_remote_code=True,
            )
            self.model.eval()  # è¯„ä¼°æ¨¡å¼
            
            # ç§»è‡³GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                self.model = self.model.to("cuda")
            
            logger.info("âœ… Jina-Reranker-v3 æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ Jina-Reranker-v3 åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def rank_documents(
        self, query: str, documents: List[str]  # ä»…ä½¿ç”¨queryï¼Œä¸ä¾èµ–task
    ) -> List[Dict]:
        """åŸºäºJina-Rerankerè¿›è¡Œç²¾æ’æ‰“åˆ†"""
        if not documents:
            return []
        
        # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
        with torch.no_grad():
            results = self.model.rerank(query, documents)
        
        # è½¬æ¢ç»“æœæ ¼å¼ï¼ˆç¡®ä¿åˆ†æ•°ä¸ºPython floatç±»å‹ï¼‰
        ranked_results = [
            {
                "document": result["document"],
                "score": round(float(result["relevance_score"]), 6)  # è½¬æ¢numpyç±»å‹ä¸ºPython float
            } 
            for result in results
        ]
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        return sorted(ranked_results, key=lambda x: x["score"], reverse=True)

# ==========================================================
#                   æ··åˆæ’åºæ§åˆ¶é€»è¾‘
# ==========================================================
class HybridReranker:
    def __init__(self):
        self.embed_model = EmbeddingReranker()
        self.reranker = JinaReranker()

    def hybrid_rank(
        self, task: str, query: str, documents: List[str], topk: int, threshold: float, normalize: bool
    ) -> List[Dict]:
        # Step 1: Embedding ç²—æ’ï¼ˆä½¿ç”¨taskä¼˜åŒ–ç²—æ’æ•ˆæœï¼‰
        logger.info("ğŸ”¹ é˜¶æ®µ1: Embedding ç²—æ’ä¸­...")
        embedding_rank = self.embed_model.rank_documents(task, query, documents, normalize)
        # å–ç²—æ’å‰3*topkä½œä¸ºç²¾æ’å€™é€‰ï¼ˆå¹³è¡¡æ•ˆç‡ä¸å¬å›ï¼‰
        candidates = embedding_rank[: min(len(embedding_rank), 3 * topk)]
        candidate_docs = [d["document"] for d in candidates]

        # Step 2: Reranker ç²¾æ’ï¼ˆä»…ç”¨queryï¼Œç¬¦åˆJinaæ¨¡å‹ç‰¹æ€§ï¼‰
        logger.info("ğŸ”¹ é˜¶æ®µ2: Reranker ç²¾æ’ä¸­...")
        reranked = self.reranker.rank_documents(query, candidate_docs)

        # Step 3: é˜ˆå€¼è¿‡æ»¤
        reranked_filtered = [d for d in reranked if d["score"] >= threshold]

        # Step 4: ä¸è¶³topkæ—¶ä»ç²—æ’ç»“æœå›è¡¥
        if len(reranked_filtered) < topk:
            existing_docs = {d["document"] for d in reranked_filtered}
            for d in embedding_rank:
                if len(reranked_filtered) >= topk:
                    break
                if d["document"] not in existing_docs:
                    reranked_filtered.append(d)
                    existing_docs.add(d["document"])

        # æœ€ç»ˆæ’åºå¹¶æˆªå–topk
        reranked_filtered.sort(key=lambda x: x["score"], reverse=True)
        return reranked_filtered[:topk]

# ==========================================================
#                   å…¨å±€å®ä¾‹ & API
# ==========================================================
hybrid_reranker = HybridReranker()


@app.post("/hybrid_rank", response_model=HybridRerankResponse)
async def hybrid_rank(request: HybridRerankRequest):
    try:
        if not request.documents:
            raise HTTPException(status_code=400, detail="documentsä¸èƒ½ä¸ºç©º")
        
        # å¤„ç†topkè¾¹ç•Œæƒ…å†µ
        effective_topk = min(request.topk, len(request.documents))
        if request.topk <= 0:
            raise HTTPException(status_code=400, detail="topkå¿…é¡»å¤§äº0")

        results = hybrid_reranker.hybrid_rank(
            task=request.task,
            query=request.query,
            documents=request.documents,
            topk=effective_topk,
            threshold=request.threshold,
            normalize=request.normalize,
        )
        return {"ranked_documents": results, "total_count": len(request.documents)}

    except Exception as e:
        logger.error(f"è¯·æ±‚å¤„ç†å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "models": ["KaLM-Embedding-2.5", "Jina-Reranker-v3"]}

@app.on_event("shutdown")
def shutdown_event():
    logger.info("ğŸ§¹ æ­£åœ¨é‡Šæ”¾æ¨¡å‹èµ„æº...")
    destroy_model_parallel()
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    logger.info("âœ… æœåŠ¡å·²å®‰å…¨å…³é—­")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8074, workers=1)