import logging
import math
import gc
import time
import torch
import torch.nn.functional as F
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from vllm import LLM
from vllm.distributed.parallel_state import destroy_model_parallel

# -------------------------- æ—¥å¿—é…ç½® --------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# -------------------------- FastAPI åˆå§‹åŒ– --------------------------
app = FastAPI(
    title="Qwen3-Embedding-0.6B Reranker API",
    description="åŸºäº Qwen3-Embedding-0.6B çš„ç›¸ä¼¼åº¦æ–‡æ¡£æ’åºæ¥å£",
)

# -------------------------- è¯·æ±‚ä¸å“åº”æ¨¡å‹ --------------------------
class RerankRequest(BaseModel):
    task: str
    query: str
    documents: List[str]
    topk: Optional[int] = 5
    normalize: Optional[bool] = True

class RerankResponse(BaseModel):
    ranked_documents: List[Dict]
    total_count: int

# -------------------------- æ¨¡å‹å°è£… --------------------------
class EmbeddingReranker:
    def __init__(self):
        self.model = None
        self.initialize()

    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            logger.info("ğŸš€ å¼€å§‹åŠ è½½ Qwen3-Embedding-0.6B æ¨¡å‹...")
            num_gpus = torch.cuda.device_count()
            self.model = LLM(
                model="/root/cyx/model_weights/Qwen3-Embedding-0.6B",
                task="embed",
                tensor_parallel_size=num_gpus if num_gpus > 0 else 1,
                gpu_memory_utilization=0.15,
            )
            logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def embed(self, task: str, texts: List[str], normalize: bool = True) -> torch.Tensor:
        """ç”Ÿæˆembeddingå¹¶è¿”å›tensor"""
        try:
            inputs = [f"Instruct: {task}\nQuery: {t}" for t in texts]
            outputs = self.model.embed(inputs)
            embeddings = [o.outputs.embedding for o in outputs]
            emb_tensor = torch.tensor(embeddings)
            if normalize:
                emb_tensor = F.normalize(emb_tensor, p=2, dim=1)
            return emb_tensor
        except Exception as e:
            logger.error(f"ç”Ÿæˆembeddingå¤±è´¥: {str(e)}")
            raise

    def rank_documents(
        self,
        task: str,
        query: str,
        documents: List[str],
        topk: Optional[int] = 5,
        normalize: bool = True,
    ) -> List[Dict]:
        """è®¡ç®—ç›¸ä¼¼åº¦å¹¶è¿”å›å‰topkæ–‡æ¡£"""
        if not documents:
            return []

        # ç”Ÿæˆquery + documentsçš„å‘é‡
        inputs = [f"Instruct: {task}\nQuery: {query}"] + documents
        embeddings = self.embed(task, inputs, normalize)
        query_emb = embeddings[0].unsqueeze(0)
        doc_embs = embeddings[1:]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        scores = (query_emb @ doc_embs.T).squeeze(0)
        scores = scores.tolist()

        # æ’åº
        ranked = sorted(
            [{"document": doc, "score": round(float(score), 6)} for doc, score in zip(documents, scores)],
            key=lambda x: x["score"],
            reverse=True,
        )

        # è¿”å›å‰topk
        if topk and topk > 0:
            ranked = ranked[:topk]
        return ranked


# -------------------------- å…¨å±€æ¨¡å‹å®ä¾‹ --------------------------
reranker = EmbeddingReranker()

# -------------------------- API è·¯ç”± --------------------------
@app.post("/rank_documents", response_model=RerankResponse)
async def rank_documents(request: RerankRequest):
    try:
        if not request.documents:
            raise HTTPException(status_code=400, detail="documentsä¸èƒ½ä¸ºç©º")
        if request.topk and (request.topk <= 0 or request.topk > len(request.documents)):
            raise HTTPException(status_code=400, detail=f"topkå¿…é¡»åœ¨1åˆ°{len(request.documents)}ä¹‹é—´")

        ranked = reranker.rank_documents(
            task=request.task,
            query=request.query,
            documents=request.documents,
            topk=request.topk,
            normalize=request.normalize,
        )

        return {"ranked_documents": ranked, "total_count": len(request.documents)}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è¯·æ±‚å¤„ç†å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model": "Qwen3-Embedding-0.6B"}

@app.on_event("shutdown")
def shutdown_event():
    logger.info("æ­£åœ¨é‡Šæ”¾æ¨¡å‹èµ„æº...")
    time.sleep(1)
    destroy_model_parallel()
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    logger.info("æœåŠ¡å·²å…³é—­ âœ…")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002, workers=1)

